{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from htmldate import find_date\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from lxml.html import fromstring\n",
    "from itertools import cycle\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from googleapiclient.discovery import build "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_RESULTS = 20\n",
    "DATA_DIR = 'data'\n",
    "OUT_DIR = 'out'\n",
    "AUTH_DIR = 'C:\\\\PycharmProjects\\\\universities_covid19_auth.txt'\n",
    "sep = os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universities = [u.strip() for u in open(f'{DATA_DIR}{sep}Universities.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key, cse_id = [l.split('=')[1].strip() for l in open(AUTH_DIR).readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_query(query, api_key, cse_id, **kwargs):\n",
    "    query_service = build(\"customsearch\", \n",
    "                          \"v1\", \n",
    "                          developerKey=api_key\n",
    "                          )  \n",
    "    query_results = query_service.cse().list(q=query,    # Query\n",
    "                                             cx=cse_id,  # CSE ID\n",
    "                                             **kwargs    \n",
    "                                             ).execute()\n",
    "    return query_results['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universitites_search = {}\n",
    "for uni in universities:\n",
    "    domain_qry = f'{uni} (Home | Home Page)'\n",
    "    qry_results = google_query(domain_qry, api_key, cse_id, num = NUM_OF_RESULTS)\n",
    "    link0 = qry_results[0]['link'].replace('www.', '')\n",
    "    uni_domain = link0[link0.find('://'):link0.find('.')][3:]\n",
    "    \n",
    "    qry = f'inurl:{uni_domain} {uni} (COVID-19| COVID 19| COVID19 | Coronavirus 2019)'\n",
    "    my_results = google_query(qry, api_key, cse_id, num = 10)\n",
    "    \n",
    "    universitites_search[uni] = []\n",
    "    for result in my_results:\n",
    "        universitites_search[uni].append(result['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for uni, urls in universitites_search.items():\n",
    "    with open(f'{OUT_DIR}{os.sep}{uni}.csv', 'w', \"utf-8\") as wr:\n",
    "        for url in urls:\n",
    "            try:\n",
    "                url = url.split('#')[0].replace('www.', '')\n",
    "\n",
    "                page = requests.get(url, timeout=5)\n",
    "                if page.status_code != 200:\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "                try:\n",
    "                    h = soup.find('h1').text.strip()\n",
    "                except:\n",
    "                    h = soup.find('title').text.strip()\n",
    "\n",
    "                if h is None or h.strip() == '':\n",
    "                    h = 'COVID-19 Updates.'\n",
    "\n",
    "                date = find_date(url)\n",
    "\n",
    "                line = f\"{url} , {h} , {date if date else datetime.today().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "                print(line)\n",
    "                wr.write(line + '\\n')\n",
    "                wr.flush()\n",
    "            except:\n",
    "                print('### ERROR! ',url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(universitites_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_domain(uni):\n",
    "#     domain_qry = f'{uni} (Home | Home Page)'\n",
    "#     for url in search(domain_qry,        # The query you want to run\n",
    "#                 tld = 'com',  # The top level domain\n",
    "#                 lang = 'en',  # The language\n",
    "#                 num = 1,     # Number of results per page\n",
    "#                 start = 0,    # First result to retrieve\n",
    "#                 stop = 1,  # Last result to retrieve\n",
    "#                 pause = rd.choice(delays),  # Lapse between HTTP requests\n",
    "#                 user_agent=rd.choice(user_agents)\n",
    "#                ):\n",
    "#         url = url.replace('www.', '')\n",
    "#         return url[url.find('://'):url.find('.')][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for uni in universities:\n",
    "#     with open(f'{OUT_DIR}{os.sep}{uni}.csv', 'w') as wr:\n",
    "#         domain = get_domain(uni)\n",
    "#         qry = f'inurl:{domain} {uni} (COVID-19| COVID 19| COVID19 | Coronavirus 2019)'\n",
    "#         print(f'##### inurl:{domain} {uni} #######################')\n",
    "        \n",
    "#         for url in search(qry,        # The query you want to run\n",
    "#                     tld = 'com',  # The top level domain\n",
    "#                     lang = 'en',  # The language\n",
    "#                     num = 5,     # Number of results per page\n",
    "#                     start = 0,    # First result to retrieve\n",
    "#                     stop = 10,  # Last result to retrieve\n",
    "#                     pause = rd.choice(delays),  # Lapse between HTTP requests\n",
    "#                     user_agent=rd.choice(user_agents)\n",
    "#                    ):\n",
    "#             try:\n",
    "#                 url = url.split('#')[0].replace('www.', '')\n",
    "                \n",
    "\n",
    "#                 page = requests.get(url, timeout=5)\n",
    "#                 if page.status_code != 200:\n",
    "#                     continue\n",
    "\n",
    "#                 soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "#                 title = soup.find('title').text.strip()\n",
    "#                 h1 = soup.find('h1').text.strip()\n",
    "#                 date = find_date(url)\n",
    "\n",
    "#                 line = f\"{url} , {title} , {date if date else datetime.today().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "#                 wr.write(line + '\\n')\n",
    "#                 wr.flush()\n",
    "\n",
    "#                 print(line)\n",
    "#                 time.sleep(rd.choice(delays))\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print('Error:', url, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
